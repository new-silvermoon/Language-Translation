{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Language_Translation_word2vec.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"SvHErNxwZwSR","colab_type":"code","outputId":"189e15ec-4239-4304-9147-4221190fca62","executionInfo":{"status":"ok","timestamp":1565001995120,"user_tz":-330,"elapsed":14538,"user":{"displayName":"Commerce CX","photoUrl":"","userId":"06603049534156734826"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["# #region Installation\n","# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","# !apt-get update -qq 2>&1 > /dev/null\n","# !apt-get -y install -qq google-drive-ocamlfuse fuse\n","# #region Google Authentication\n","# from google.colab import auth\n","# auth.authenticate_user()\n","# from oauth2client.client import GoogleCredentials\n","# creds = GoogleCredentials.get_application_default()\n","# import getpass\n","# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","# vcode = getpass.getpass()\n","# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","# !mkdir -p drive\n","# !google-drive-ocamlfuse drive\n","\n","import keras\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense,Embedding\n","import numpy as np\n","import codecs\n","import argparse\n","from sklearn.externals import joblib\n","import pandas as pd\n","import pickle\n","\n","\n","\n","\n","class LanguageTranslation:\n","\n","    def __init__(self):\n","        #default path for the folder inside google drive\n","        default_path = \"drive/\"\n","        #path for training text (article)\n","        train_text_path = default_path + \"data/translation/fra.txt\"\n","        #path for testing text (article)\n","        test_text_path = default_path + \"data/test/translation/test.txt\"\n","        #path for saving trained models\n","        saved_models_path = default_path + \"saved_model/translation/\"\n","\n","        \n","       \n","        self.path = train_text_path\n","        self.test_path = test_text_path\n","        self.epochs = 50\n","        self.batch_size = 32\n","        self.latent_dim = 128\n","        self.embedding_dim = 128\n","        self.num_samples = 10000\n","        self.outdir = saved_models_path\n","        self.verbose = 0\n","\n","        if self.verbose == 1:\n","            self.verbose = True\n","        else:\n","            self.verbose = False\n","        self.mode = \"tn\" \n","\n","    def read_input_file(self,path,num_samples=10e13):\n","        input_texts = []\n","        target_texts = []\n","        input_words = set()\n","        target_words = set()\n","\n","        \n","        \n","        with codecs.open(path, 'r', encoding='utf-8') as f:\n","            lines = f.read().split('\\n')\n","\n","        for line in lines[: min(num_samples, len(lines) - 1)]:\n","            input_text, target_text = line.split('\\t')  # \\t as the start of sequence \n","            target_text = '\\t ' + target_text + ' \\n'   # \\n as the end  of sequence\n","            input_texts.append(input_text)\n","            target_texts.append(target_text)\n","            for word in input_text.split(\" \"):\n","                if word not in input_words:\n","                    input_words.add(word)\n","            for word in target_text.split(\" \"):\n","                if word not in target_words:\n","                    target_words.add(word)\n","\n","        return input_texts,target_texts,input_words,target_words\n","        \n","\n","    def vocab_generation(self,path,num_samples,verbose=True):\n","        \n","        input_texts,target_texts,input_words,target_words = self.read_input_file(path,num_samples)\n","        input_words = sorted(list(input_words))\n","        target_words = sorted(list(target_words))\n","        self.num_encoder_words = len(input_words)\n","        self.num_decoder_words = len(target_words)\n","        self.max_encoder_seq_length = max([len(txt.split(\" \")) for txt in input_texts])\n","        self.max_decoder_seq_length = max([len(txt.split(\" \")) for txt in target_texts])\n","\n","        if verbose == True:\n","        \n","            print('Number of samples:', len(input_texts))\n","            print('Number of unique input tokens:', self.num_encoder_words)\n","            print('Number of unique output tokens:', self.num_decoder_words)\n","            print('Max sequence length for inputs:', self.max_encoder_seq_length)\n","            print('Max sequence length for outputs:', self.max_decoder_seq_length)\n","        \n","        self.input_word_index = dict(\n","            [(word, i) for i, word in enumerate(input_words)])\n","        self.target_word_index = dict(\n","            [(word, i) for i, word in enumerate(target_words)])\n","        self.reverse_input_word_dict = dict(\n","            (i, word) for word, i in self.input_word_index.items())\n","        self.reverse_target_word_dict = dict(\n","            (i, word) for word, i in self.target_word_index.items())\n","        \n","   \n","    def process_input(self,input_texts,target_texts=None,verbose=True):\n","\n","        encoder_input_data = np.zeros(\n","            (len(input_texts), self.max_encoder_seq_length),\n","            dtype='float32')\n","            \n","        decoder_input_data = np.zeros(\n","            (len(input_texts), self.max_decoder_seq_length),\n","            dtype='float32')\n","\n","        decoder_target_data = np.zeros(\n","            (len(input_texts), self.max_decoder_seq_length,1),\n","            dtype='float32')\n","            \n","        if self.mode == 'train':\n","            for i, (input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n","                for t, word in enumerate(input_text.split(\" \")):\n","                    try:\n","                        encoder_input_data[i, t] = self.input_word_index[word]\n","                    except:\n","                        encoder_input_data[i, t] = self.num_encoder_words  \n","                        \n","                for t, word in enumerate(target_text.split(\" \")):\n","                    try:\n","                        decoder_input_data[i, t] = self.target_word_index[word]\n","                    except:\n","                        decoder_input_data[i, t] = self.num_decoder_words \n","                    if t > 0:\n","                        try:\n","                            decoder_target_data[i, t - 1] = self.target_word_index[word]\n","                        except:\n","                            decoder_target_data[i, t - 1] = self.num_decoder_words  \n","            print(self.num_encoder_words)\n","            print(self.num_decoder_words)\n","            print(self.embedding_dim)\n","            self.english_emb = np.zeros((self.num_encoder_words + 1,self.embedding_dim))\n","            self.french_emb = np.zeros((self.num_decoder_words + 1,self.embedding_dim))\n","            return encoder_input_data,decoder_input_data,decoder_target_data,np.array(input_texts),np.array(target_texts)\n","        else:\n","            for i, input_text in enumerate(input_texts):\n","                for t, word in enumerate(input_text.split(\" \")):\n","                    try:\n","                        encoder_input_data[i, t] = self.input_word_index[word]\n","                    except:\n","                        encoder_input_data[i, t] = self.num_encoder_words  \n","\n","                    \n","\n","\n","            return encoder_input_data,None,None,np.array(input_texts),None\n","\n","\n","    def train_test_split(self,num_recs,train_frac=0.8):\n","        rec_indices = np.arange(num_recs)\n","        np.random.shuffle(rec_indices)\n","        train_count = int(num_recs*0.8)\n","        train_indices =  rec_indices[:train_count]\n","        test_indices =  rec_indices[train_count:]\n","        return train_indices,test_indices\n","\n","    def model_enc_dec(self):\n","        #Encoder Model\n","        encoder_inp = Input(shape=(None,),name='encoder_inp')\n","        encoder_inp1 = Embedding(self.num_encoder_words + 1 ,self.embedding_dim,weights=[self.english_emb])(encoder_inp)\n","        encoder = LSTM(self.latent_dim, return_state=True,name='encoder')\n","        encoder_out,state_h, state_c = encoder(encoder_inp1)\n","        encoder_states = [state_h, state_c]\n","\n","        #Decoder Model\n","        decoder_inp = Input(shape=(None,),name='decoder_inp')\n","        decoder_inp1 = Embedding(self.num_decoder_words +1 ,self.embedding_dim,weights=[self.french_emb])(decoder_inp)\n","        decoder_lstm = LSTM(self.latent_dim, return_sequences=True, return_state=True,name='decoder_lstm')\n","        decoder_out, _, _ = decoder_lstm(decoder_inp1,initial_state=encoder_states)\n","        decoder_dense = Dense(self.num_decoder_words, activation='softmax',name='decoder_dense')\n","        decoder_out = decoder_dense(decoder_out)\n","        print(np.shape(decoder_out))\n","        #Combined Encoder Decoder Model\n","        model  = Model([encoder_inp, decoder_inp], decoder_out)\n","        #Encoder Model \n","        encoder_model = Model(encoder_inp,encoder_states)\n","        #Decoder Model\n","        decoder_inp_h = Input(shape=(self.latent_dim,))\n","        decoder_inp_c = Input(shape=(self.latent_dim,))\n","        decoder_inp_state = [decoder_inp_h,decoder_inp_c]\n","        decoder_out,decoder_out_h,decoder_out_c = decoder_lstm(decoder_inp1,initial_state=decoder_inp_state)\n","        decoder_out = decoder_dense(decoder_out)\n","        decoder_out_state = [decoder_out_h,decoder_out_c]\n","        decoder_model = Model(inputs = [decoder_inp] + decoder_inp_state,output=[decoder_out]+ decoder_out_state)\n","\n","        return model,encoder_model,decoder_model\n","\n","\n","    def decode_sequence(self,input_seq,encoder_model,decoder_model):\n","        # Encode the input as state vectors.\n","        states_value = encoder_model.predict(input_seq)\n","\n","        target_seq = np.zeros((1, 1))\n","\n","        target_seq[0, 0] = self.target_word_index['\\t']\n","\n","        stop_condition = False\n","        decoded_sentence = ''\n","\n","        while not stop_condition:\n","            output_word, h, c = decoder_model.predict(\n","                [target_seq] + states_value)\n","\n","            sampled_word_index = np.argmax(output_word[0, -1, :])\n","            try:\n","                sampled_char = self.reverse_target_word_dict[sampled_word_index]\n","            except:\n","                sampled_char = '<unknown>'\n","            decoded_sentence = decoded_sentence + ' ' + sampled_char\n","\n","            if (sampled_char == '\\n' or\n","            len(decoded_sentence) > self.max_decoder_seq_length):\n","                stop_condition = True\n","\n","            target_seq = np.zeros((1, 1))\n","            target_seq[0, 0] = sampled_word_index\n","\n","\n","            states_value = [h, c]\n","\n","        return decoded_sentence\n","\n","\n","\n","    def train(self,encoder_input_data,decoder_input_data,decoder_target_data):\n","        print(\"Training...\")\n","        \n","        print(np.shape(encoder_input_data))\n","        print(np.shape(decoder_input_data))\n","        print(np.shape(decoder_target_data))\n","\n","        model,encoder_model,decoder_model = self.model_enc_dec()\n","\n","        model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","        model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","                batch_size=self.batch_size,\n","                epochs=self.epochs,\n","                validation_split=0.2)\n","        # Save model\n","        model.save(self.outdir + 'eng_2_french_dumm.h5')\n","        return model,encoder_model,decoder_model\n","\n","    def inference(self,model,data,encoder_model,decoder_model,in_text):\n","        in_list,out_list = [],[]\n","        for seq_index in range(data.shape[0]):\n","\n","            input_seq = data[seq_index: seq_index + 1]\n","            decoded_sentence = self.decode_sequence(input_seq,encoder_model,decoder_model)\n","            print('-')\n","            print('Input sentence:', in_text[seq_index])\n","            print('Decoded sentence:',decoded_sentence)\n","            in_list.append(in_text[seq_index])\n","            out_list.append(decoded_sentence)\n","        return in_list,out_list\n","    \n","    def save_models(self,outdir):\n","        self.model.save(outdir + 'enc_dec_model.h5')\n","        self.encoder_model.save(outdir + 'enc_model.h5')\n","        self.decoder_model.save(outdir + 'dec_model.h5')\n","        \n","        variables_store = {'num_encoder_words':self.num_encoder_words,\n","                        'num_decoder_words':self.num_decoder_words,\n","                        'max_encoder_seq_length':self.max_encoder_seq_length,\n","                        'max_decoder_seq_length':self.max_decoder_seq_length,\n","                        'input_word_index':self.input_word_index,\n","                        'target_word_index':self.target_word_index,\n","                        'reverse_input_word_dict':self.reverse_input_word_dict,\n","                        'reverse_target_word_dict':self.reverse_target_word_dict\n","                        }\n","        with open(outdir + 'variable_store.pkl','wb') as f:\n","            pickle.dump(variables_store,f)\n","            f.close()\n","\n","\n","    def load_models(self,outdir):\n","        self.model = keras.models.load_model(outdir + 'enc_dec_model.h5')\n","        self.encoder_model = keras.models.load_model(outdir + 'enc_model.h5')\n","        self.decoder_model = keras.models.load_model(outdir + 'dec_model.h5')\n","        \n","        with open(outdir + 'variable_store.pkl','rb') as f:\n","            variables_store = pickle.load(f)\n","            f.close()\n","\n","        self.num_encoder_words = variables_store['num_encoder_words']\n","        self.num_decoder_words = variables_store['num_decoder_words']\n","        self.max_encoder_seq_length = variables_store['max_encoder_seq_length']\n","        self.max_decoder_seq_length = variables_store['max_decoder_seq_length']\n","        self.input_word_index = variables_store['input_word_index']\n","        self.target_word_index = variables_store['target_word_index']\n","        self.reverse_input_word_dict = variables_store['reverse_input_word_dict']\n","        self.reverse_target_word_dict = variables_store['reverse_target_word_dict']\n","        \n","    def main(self):\n","\n","        if self.mode == 'train':\n","            self.vocab_generation(self.path,self.num_samples,self.verbose) # Generate the vocabulary\n","            input_texts,target_texts,_,_ = self.read_input_file(self.path,self.num_samples)\n","            encoder_input_data,decoder_input_data,decoder_target_data,input_texts,target_texts = \\\n","                                                 self.process_input(input_texts,target_texts,True)\n","            num_recs =  encoder_input_data.shape[0]\n","            train_indices,test_indices = self.train_test_split(num_recs,0.8)\n","            encoder_input_data_tr,encoder_input_data_te = encoder_input_data[train_indices,],encoder_input_data[test_indices,]\n","            decoder_input_data_tr,decoder_input_data_te = decoder_input_data[train_indices,],decoder_input_data[test_indices,]\n","            decoder_target_data_tr,decoder_target_data_te = decoder_target_data[train_indices,],decoder_target_data[test_indices,]\n","            input_text_tr,input_text_te = input_texts[train_indices],input_texts[test_indices]                                                      \n","            self.model,self.encoder_model,self.decoder_model = self.train(encoder_input_data_tr,decoder_input_data_tr,decoder_target_data_tr)\n","            in_list,out_list = self.inference(self.model,encoder_input_data_te,self.encoder_model,self.decoder_model,input_text_te)\n","            out_df = pd.DataFrame()\n","            out_df['English text'] = in_list\n","            out_df['French text'] = out_list\n","            out_df.to_csv(self.outdir + 'hold_out_results_validation.csv',index=False)\n","            self.save_models(self.outdir)\n","                    \n","        else:\n","            self.load_models(self.outdir)\n","            input_texts,_,_,_ = self.read_input_file(self.test_path,self.num_samples)\n","            encoder_input_data,_,_,input_texts,_ = \\\n","                                                 self.process_input(input_texts,'',True)\n","            in_list,out_list  = self.inference(self.model,encoder_input_data,self.encoder_model,self.decoder_model,input_texts)\n","            out_df = pd.DataFrame()\n","            out_df['English text'] = in_list\n","            out_df['French text'] = out_list\n","            out_df.to_csv(self.outdir + 'results_test.csv',index=False)\n","\n","        \n","\n","        \n","if __name__ == '__main__':\n","    obj = LanguageTranslation()\n","    obj.main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"},{"output_type":"stream","text":["-\n","Input sentence: I'm thin.\n","Decoded sentence:  Je suis certain.\n","-\n","Input sentence: Hello!\n","Decoded sentence:  en Es-tu Viens\n","-\n","Input sentence: How are you.\n","Decoded sentence:  Comment va ?\n"],"name":"stdout"}]}]}